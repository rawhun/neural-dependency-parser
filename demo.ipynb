{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0270156b",
   "metadata": {},
   "source": [
    "Neural Network Dependency Parser Demo\n",
    "\n",
    "This notebook demonstrates how to use your trained dependency parser to analyze sentences and visualize dependency trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363e3c24",
   "metadata": {},
   "source": [
    "1. Setup and Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07053913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "sys.path.append(os.path.abspath('.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb63df5b",
   "metadata": {},
   "source": [
    "2. Load Model and Vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d177634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_vocabs():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    proc_dir = os.path.join('data', 'processed')\n",
    "    with open(os.path.join(proc_dir, 'word_vocab.pkl'), 'rb') as f:\n",
    "        word_vocab = pickle.load(f)\n",
    "    with open(os.path.join(proc_dir, 'pos_vocab.pkl'), 'rb') as f:\n",
    "        pos_vocab = pickle.load(f)\n",
    "    with open(os.path.join(proc_dir, 'label_vocab.pkl'), 'rb') as f:\n",
    "        label_vocab = pickle.load(f)\n",
    "    from models.parser import DependencyParser\n",
    "    model = DependencyParser(\n",
    "        vocab_sizes={'word': len(word_vocab), 'pos': len(pos_vocab)},\n",
    "        emb_dims={'word': 100, 'pos': 32},\n",
    "        lstm_dim=256,\n",
    "        num_labels=len(label_vocab)\n",
    "    ).to(device)\n",
    "    model.load_state_dict(torch.load('best_model.pt', map_location=device))\n",
    "    model.eval()\n",
    "    return model, word_vocab, pos_vocab, label_vocab, device\n",
    "\n",
    "model, word_vocab, pos_vocab, label_vocab, device = load_model_and_vocabs()\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2d0e1f",
   "metadata": {},
   "source": [
    "3. Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0716d460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(sentence)\n",
    "    words = [token.text for token in doc]\n",
    "    pos_tags = [token.pos_ for token in doc]\n",
    "    return words, pos_tags\n",
    "\n",
    "def predict_dependencies(model, words, pos_tags, word_vocab, pos_vocab, label_vocab, device):\n",
    "    word_idx = [word_vocab.get(w, word_vocab['<unk>']) for w in words]\n",
    "    pos_idx = [pos_vocab.get(p, pos_vocab['<unk>']) for p in pos_tags]\n",
    "    word_tensor = torch.tensor([word_idx], dtype=torch.long).to(device)\n",
    "    pos_tensor = torch.tensor([pos_idx], dtype=torch.long).to(device)\n",
    "    with torch.no_grad():\n",
    "        head_scores, label_scores = model(word_tensor, pos_tensor)\n",
    "        pred_heads = head_scores.argmax(-1).squeeze(0)\n",
    "        pred_labels = label_scores.permute(0,2,3,1).gather(\n",
    "            2, pred_heads.unsqueeze(-1).unsqueeze(-1).expand(-1,-1,1,label_scores.size(1))\n",
    "        ).squeeze(2).argmax(-1).squeeze(0)\n",
    "    return pred_heads.cpu().numpy(), pred_labels.cpu().numpy()\n",
    "\n",
    "def format_dependency_tree(words, pos_tags, heads, labels, label_vocab):\n",
    "    print(\"\\nDependency Tree:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'ID':<3} {'Word':<15} {'POS':<8} {'Head':<8} {'Label':<15}\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, (word, pos, head, label) in enumerate(zip(words, pos_tags, heads, labels)):\n",
    "        head_word = words[head] if head < len(words) else \"ROOT\"\n",
    "        label_name = label_vocab.itos[label] if label < len(label_vocab.itos) else \"UNK\"\n",
    "        print(f\"{i:<3} {word:<15} {pos:<8} {head_word:<8} {label_name:<15}\")\n",
    "\n",
    "def visualize_dependency_tree(words, pos_tags, heads, labels, label_vocab, title=\"Dependency Tree\"):\n",
    "    G = nx.DiGraph()\n",
    "    for i, (word, pos) in enumerate(zip(words, pos_tags)):\n",
    "        G.add_node(i, word=word, pos=pos)\n",
    "    for i, (head, label) in enumerate(zip(heads, labels)):\n",
    "        if head < len(words):\n",
    "            label_name = label_vocab.itos[label] if label < len(label_vocab.itos) else \"UNK\"\n",
    "            G.add_edge(head, i, label=label_name)\n",
    "    pos = nx.spring_layout(G, k=3, iterations=50)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    nx.draw(G, pos, with_labels=True, node_color='lightblue',\n",
    "            node_size=2000, font_size=10, font_weight='bold',\n",
    "            arrows=True, arrowstyle='->', arrowsize=20)\n",
    "    edge_labels = nx.get_edge_attributes(G, 'label')\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)\n",
    "    node_labels = {i: f\"{G.nodes[i]['word']}\\n({G.nodes[i]['pos']})\" for i in G.nodes()}\n",
    "    nx.draw_networkx_labels(G, pos, node_labels, font_size=8)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ebb169",
   "metadata": {},
   "source": [
    "4. Example Sentence Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0010a65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"I love neural networks.\",\n",
    "    \"She quickly ran to the store.\",\n",
    "    \"The beautiful red car drove fast.\"\n",
    "]\n",
    "\n",
    "for i, sentence in enumerate(test_sentences, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Sentence {i}: {sentence}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    words, pos_tags = tokenize_sentence(sentence)\n",
    "    print(f\"Tokens: {words}\")\n",
    "    print(f\"POS tags: {pos_tags}\")\n",
    "    heads, labels = predict_dependencies(model, words, pos_tags, word_vocab, pos_vocab, label_vocab, device)\n",
    "    format_dependency_tree(words, pos_tags, heads, labels, label_vocab)\n",
    "    visualize_dependency_tree(words, pos_tags, heads, labels, label_vocab, f\"Sentence {i}: {sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cee75a0",
   "metadata": {},
   "source": [
    "5. Interactive Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622163b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_custom_sentence():\n",
    "    sentence = input(\"Enter a sentence to parse: \")\n",
    "    if sentence.strip():\n",
    "        words, pos_tags = tokenize_sentence(sentence)\n",
    "        heads, labels = predict_dependencies(model, words, pos_tags, word_vocab, pos_vocab, label_vocab, device)\n",
    "        format_dependency_tree(words, pos_tags, heads, labels, label_vocab)\n",
    "        visualize_dependency_tree(words, pos_tags, heads, labels, label_vocab, f\"Custom: {sentence}\")\n",
    "\n",
    "# Uncomment to use:\n",
    "# parse_custom_sentence()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
