{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a5dba7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Neural Network Dependency Parser Demo\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook demonstrates how to use the trained neural dependency parser to analyze sentences and visualize dependency trees.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import sys\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"sys.path.append(os.path.dirname(os.path.abspath('.')))\\n\",\n",
    "    \"\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"import pickle\\n\",\n",
    "    \"import spacy\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import networkx as nx\\n\",\n",
    "    \"from models.parser import DependencyParser\\n\",\n",
    "    \"from models.vocab import Vocab\\n\",\n",
    "    \"from train import load_vocabs\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Dependencies loaded successfully!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Load the Trained Model\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def load_model_and_vocabs():\\n\",\n",
    "    \"    \\\"\\\"\\\"Load the trained model and vocabularies.\\\"\\\"\\\"\\n\",\n",
    "    \"    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\n\",\n",
    "    \"    proc_dir = os.path.join('data', 'processed')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Load vocabularies\\n\",\n",
    "    \"    word_vocab, pos_vocab, label_vocab = load_vocabs(proc_dir)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Initialize model\\n\",\n",
    "    \"    model = DependencyParser(\\n\",\n",
    "    \"        vocab_sizes={'word': len(word_vocab), 'pos': len(pos_vocab)},\\n\",\n",
    "    \"        emb_dims={'word': 100, 'pos': 32},\\n\",\n",
    "    \"        lstm_dim=256,\\n\",\n",
    "    \"        num_labels=len(label_vocab)\\n\",\n",
    "    \"    ).to(device)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Load trained weights\\n\",\n",
    "    \"    model.load_state_dict(torch.load('best_model.pt', map_location=device))\\n\",\n",
    "    \"    model.eval()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return model, word_vocab, pos_vocab, label_vocab, device\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load model\\n\",\n",
    "    \"print(\\\"Loading model and vocabularies...\\\")\\n\",\n",
    "    \"model, word_vocab, pos_vocab, label_vocab, device = load_model_and_vocabs()\\n\",\n",
    "    \"print(f\\\"Model loaded successfully on {device}!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Define Helper Functions\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def tokenize_sentence(sentence):\\n\",\n",
    "    \"    \\\"\\\"\\\"Tokenize and POS tag a sentence using spaCy.\\\"\\\"\\\"\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        nlp = spacy.load(\\\"en_core_web_sm\\\")\\n\",\n",
    "    \"    except OSError:\\n\",\n",
    "    \"        print(\\\"spaCy English model not found. Installing...\\\")\\n\",\n",
    "    \"        os.system(\\\"python -m spacy download en_core_web_sm\\\")\\n\",\n",
    "    \"        nlp = spacy.load(\\\"en_core_web_sm\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    doc = nlp(sentence)\\n\",\n",
    "    \"    words = [token.text for token in doc]\\n\",\n",
    "    \"    pos_tags = [token.pos_ for token in doc]\\n\",\n",
    "    \"    return words, pos_tags\\n\",\n",
    "    \"\\n\",\n",
    "    \"def predict_dependencies(model, words, pos_tags, word_vocab, pos_vocab, label_vocab, device):\\n\",\n",
    "    \"    \\\"\\\"\\\"Predict dependency heads and labels for a sentence.\\\"\\\"\\\"\\n\",\n",
    "    \"    # Convert to indices\\n\",\n",
    "    \"    word_idx = [word_vocab.get(w, word_vocab['<unk>']) for w in words]\\n\",\n",
    "    \"    pos_idx = [pos_vocab.get(p, pos_vocab['<unk>']) for p in pos_tags]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Convert to tensors\\n\",\n",
    "    \"    word_tensor = torch.tensor([word_idx], dtype=torch.long).to(device)\\n\",\n",
    "    \"    pos_tensor = torch.tensor([pos_idx], dtype=torch.long).to(device)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Predict\\n\",\n",
    "    \"    with torch.no_grad():\\n\",\n",
    "    \"        head_scores, label_scores = model(word_tensor, pos_tensor)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Get predictions\\n\",\n",
    "    \"        pred_heads = head_scores.argmax(-1).squeeze(0)  # (seq_len,)\\n\",\n",
    "    \"        pred_labels = label_scores.permute(0,2,3,1).gather(\\n\",\n",
    "    \"            2, pred_heads.unsqueeze(-1).unsqueeze(-1).expand(-1,-1,1,label_scores.size(1))\\n\",\n",
    "    \"        ).squeeze(2).argmax(-1).squeeze(0)  # (seq_len,)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return pred_heads.cpu().numpy(), pred_labels.cpu().numpy()\\n\",\n",
    "    \"\\n\",\n",
    "    \"def visualize_dependency_tree(words, pos_tags, heads, labels, label_vocab, title=\\\"Dependency Tree\\\"):\\n\",\n",
    "    \"    \\\"\\\"\\\"Visualize the dependency tree using networkx and matplotlib.\\\"\\\"\\\"\\n\",\n",
    "    \"    # Create graph\\n\",\n",
    "    \"    G = nx.DiGraph()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Add nodes\\n\",\n",
    "    \"    for i, (word, pos) in enumerate(zip(words, pos_tags)):\\n\",\n",
    "    \"        G.add_node(i, word=word, pos=pos)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Add edges\\n\",\n",
    "    \"    for i, (head, label) in enumerate(zip(heads, labels)):\\n\",\n",
    "    \"        if head < len(words):  # Valid head index\\n\",\n",
    "    \"            label_name = label_vocab.itos[label] if label < len(label_vocab.itos) else \\\"UNK\\\"\\n\",\n",
    "    \"            G.add_edge(head, i, label=label_name)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Create layout\\n\",\n",
    "    \"    pos = nx.spring_layout(G, k=3, iterations=50)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Draw the graph\\n\",\n",
    "    \"    plt.figure(figsize=(12, 8))\\n\",\n",
    "    \"    nx.draw(G, pos, with_labels=True, node_color='lightblue', \\n\",\n",
    "    \"            node_size=2000, font_size=10, font_weight='bold',\\n\",\n",
    "    \"            arrows=True, arrowstyle='->', arrowsize=20)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Add edge labels\\n\",\n",
    "    \"    edge_labels = nx.get_edge_attributes(G, 'label')\\n\",\n",
    "    \"    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Add node labels\\n\",\n",
    "    \"    node_labels = {i: f\\\"{G.nodes[i]['word']}\\\\n({G.nodes[i]['pos']})\\\" for i in G.nodes()}\\n\",\n",
    "    \"    nx.draw_networkx_labels(G, pos, node_labels, font_size=8)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.title(title)\\n\",\n",
    "    \"    plt.axis('off')\\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"def compare_with_spacy(sentence):\\n\",\n",
    "    \"    \\\"\\\"\\\"Compare our parser with spaCy's parser.\\\"\\\"\\\"\\n\",\n",
    "    \"    print(f\\\"Sentence: {sentence}\\\")\\n\",\n",
    "    \"    print(\\\"=\\\"*60)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Our parser\\n\",\n",
    "    \"    print(\\\"\\\\nOur Neural Parser:\\\")\\n\",\n",
    "    \"    words, pos_tags = tokenize_sentence(sentence)\\n\",\n",
    "    \"    heads, labels = predict_dependencies(model, words, pos_tags, word_vocab, pos_vocab, label_vocab, device)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(\\\"Dependencies:\\\")\\n\",\n",
    "    \"    for i, (word, pos, head, label) in enumerate(zip(words, pos_tags, heads, labels)):\\n\",\n",
    "    \"        head_word = words[head] if head < len(words) else \\\"ROOT\\\"\\n\",\n",
    "    \"        label_name = label_vocab.itos[label] if label < len(label_vocab.itos) else \\\"UNK\\\"\\n\",\n",
    "    \"        print(f\\\"  {word} ({pos}) -> {head_word} ({label_name})\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # spaCy parser\\n\",\n",
    "    \"    print(\\\"\\\\nspaCy Parser:\\\")\\n\",\n",
    "    \"    nlp = spacy.load(\\\"en_core_web_sm\\\")\\n\",\n",
    "    \"    doc = nlp(sentence)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(\\\"Dependencies:\\\")\\n\",\n",
    "    \"    for token in doc:\\n\",\n",
    "    \"        print(f\\\"  {token.text} ({token.pos_}) -> {token.head.text} ({token.dep_})\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Visualize our parser\\n\",\n",
    "    \"    visualize_dependency_tree(words, pos_tags, heads, labels, label_vocab, \\\"Our Neural Parser\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Visualize spaCy parser\\n\",\n",
    "    \"    spacy_words = [token.text for token in doc]\\n\",\n",
    "    \"    spacy_pos = [token.pos_ for token in doc]\\n\",\n",
    "    \"    spacy_heads = [token.head.i for token in doc]\\n\",\n",
    "    \"    spacy_labels = [token.dep_ for token in doc]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Create a simple label mapping for visualization\\n\",\n",
    "    \"    spacy_label_vocab = type('obj', (object,), {\\n\",\n",
    "    \"        'itos': list(set(spacy_labels))\\n\",\n",
    "    \"    })()\\n\",\n",
    "    \"    spacy_label_indices = [spacy_label_vocab.itos.index(label) for label in spacy_labels]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    visualize_dependency_tree(spacy_words, spacy_pos, spacy_heads, spacy_label_indices, spacy_label_vocab, \\\"spaCy Parser\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Test Example Sentences\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Test sentences\\n\",\n",
    "    \"test_sentences = [\\n\",\n",
    "    \"    \\\"The cat sat on the mat.\\\",\\n\",\n",
    "    \"    \\\"I love neural networks.\\\",\\n\",\n",
    "    \"    \\\"She quickly ran to the store.\\\",\\n\",\n",
    "    \"    \\\"The beautiful red car drove fast.\\\",\\n\",\n",
    "    \"    \\\"John gave Mary a book yesterday.\\\"\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"for sentence in test_sentences:\\n\",\n",
    "    \"    compare_with_spacy(sentence)\\n\",\n",
    "    \"    print(\\\"\\\\n\\\" + \\\"=\\\"*80 + \\\"\\\\n\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Interactive Parsing\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def parse_custom_sentence():\\n\",\n",
    "    \"    \\\"\\\"\\\"Parse a custom sentence entered by the user.\\\"\\\"\\\"\\n\",\n",
    "    \"    sentence = input(\\\"Enter a sentence to parse: \\\")\\n\",\n",
    "    \"    if sentence.strip():\\n\",\n",
    "    \"        compare_with_spacy(sentence)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Uncomment the line below to enable interactive parsing\\n\",\n",
    "    \"# parse_custom_sentence()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Performance Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def analyze_parser_performance():\\n\",\n",
    "    \"    \\\"\\\"\\\"Analyze the parser's performance on different sentence types.\\\"\\\"\\\"\\n\",\n",
    "    \"    sentence_types = {\\n\",\n",
    "    \"        \\\"Simple\\\": [\\\"The dog barked.\\\", \\\"I am happy.\\\", \\\"She sings.\\\"],\\n\",\n",
    "    \"        \\\"Complex\\\": [\\\"The cat that sat on the mat is sleeping.\\\", \\n\",\n",
    "    \"                    \\\"I believe that he will come tomorrow.\\\",\\n\",\n",
    "    \"                    \\\"The book that I read yesterday was interesting.\\\"],\\n\",\n",
    "    \"        \\\"Questions\\\": [\\\"What did you eat?\\\", \\\"Where is the cat?\\\", \\\"How are you?\\\"],\\n\",\n",
    "    \"        \\\"Negatives\\\": [\\\"I don't like it.\\\", \\\"She won't come.\\\", \\\"They can't see.\\\"]\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for sentence_type, sentences in sentence_types.items():\\n\",\n",
    "    \"        print(f\\\"\\\\n{sentence_type} Sentences:\\\")\\n\",\n",
    "    \"        print(\\\"-\\\" * 40)\\n\",\n",
    "    \"        for sentence in sentences:\\n\",\n",
    "    \"            print(f\\\"\\\\nParsing: {sentence}\\\")\\n\",\n",
    "    \"            words, pos_tags = tokenize_sentence(sentence)\\n\",\n",
    "    \"            heads, labels = predict_dependencies(model, words, pos_tags, word_vocab, pos_vocab, label_vocab, device)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Print key dependencies\\n\",\n",
    "    \"            for i, (word, head, label) in enumerate(zip(words, heads, labels)):\\n\",\n",
    "    \"                if head < len(words):\\n\",\n",
    "    \"                    head_word = words[head]\\n\",\n",
    "    \"                    label_name = label_vocab.itos[label] if label < len(label_vocab.itos) else \\\"UNK\\\"\\n\",\n",
    "    \"                    print(f\\\"  {word} -> {head_word} ({label_name})\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Uncomment to run performance analysis\\n\",\n",
    "    \"# analyze_parser_performance()\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.8.0\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
